{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "178b615a-ca81-43f5-b370-044275d956fa",
   "metadata": {},
   "source": [
    "### A. Data Cleaning \n",
    "\n",
    "**Instructions:**\n",
    "1. Import `pandas` and `numpy`. Import `SimpleImputer` from `sklearn.impute`. Import `StandardScaler` from `sklearn.preprocessing`. Import `train_test_split` from \n",
    "`sklearn.model_selection`. \n",
    "2. Use `read_csv` to load `AmesHousing.csv` as `housing`. \n",
    "3. Use the `.select_dtypes` and `.columns` commands to create `numerical_cols`, a list containing all of the `float64`, `int64` columns.\n",
    "4. Use the `.select_dtypes` and `.columns` commands to create `numerical_cols`, a list containing all of the `object` columns.\n",
    "5. Define `num_imputer` as `SimpleImputer` with a `median` strategy. Define `cat_imputer` as `SimpleImputer(strategy='constant', fill_value='missing')`. \n",
    "6. Replace the `numerical_cols` of `housing` with imputed columns using the `.fit_transform` function of `num_imputer` on the numerical columns of the `housing` dataset.\n",
    "7. Replace the `categorical_cols` of `housing` with imputed columns using the `.fit_transform` function of `cat_imputer` on the categorical columns of the `housing` dataset.\n",
    "8. Define a new list of `categorical_columns` using `.select_dtypes` and `.columns`, searching for all of the `object` columns. Now, use this list of categorical columns in the `columns` argument of `.get_dummies` to create a new dataset `housing_dummies` that contains all of the imputed columns, with the categorical columns having been converted to dummy variables. \n",
    "9. Define label variable `y` as `SalePrice` and feature matrix `X` as all other variables.\n",
    "10. Use the `.fit_transform()` function of the `StandardScaler` on `X` to create `X_scaled`\n",
    "11. Divide the data into `X_train`, `X_test`, `y_train`, `y_test` using  `train_test_split` with 20% of the data being testing. For replicability, fix a random state. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97539326-288f-4837-8b2d-3a1bc1b9103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Data \n",
    "housing = pd.read_csv('AmesHousing.csv')\n",
    "numerical_cols = housing.select_dtypes(['float', 'int64']).columns\n",
    "categorical_cols = housing.select_dtypes(['object']).columns\n",
    "# Impute Missing Values \n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "# 6 and 7 Replacing the two columns\n",
    "housing[numerical_cols] = num_imputer.fit_transform(housing[numerical_cols])\n",
    "housing[categorical_cols] = cat_imputer.fit_transform(housing[categorical_cols])\n",
    "\n",
    "# Convert categorical variables to dummies \n",
    "categorical_cols_new = housing.select_dtypes(include=['object']).columns\n",
    "housing_dummies = pd.get_dummies(housing, columns=categorical_cols_new)\n",
    "\n",
    "# Define the label and feature \n",
    "y = housing_dummies['SalePrice']\n",
    "X = housing_dummies.drop('SalePrice', axis=1)\n",
    "# Normalize features \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a469f2-4f88-41c9-8703-720930e3e090",
   "metadata": {},
   "source": [
    "### B. Define Models and Hyperparameter Grid\n",
    "\n",
    "**Instructions:**\n",
    "1. Import the following regression models from the `linear_model` module of scikit-learn:\n",
    "   - `Lasso`\n",
    "   - `Ridge`\n",
    "   - `ElasticNet`\n",
    "   - `LinearRegression`\n",
    "2. Create a dictionary named `models` to store instances of `Lasso`, `Ridge`, and `ElasticNet` where the key is the model name and the value is the model instance.\n",
    "3. Define a dictionary named `param_grid` that specifies the hyperparameter search space for each model:\n",
    "   - `Lasso`: `alpha` values of `[10, 100, 1000, 10000]` and `max_iter` set to `[10000]`.\n",
    "   - `Ridge`: `alpha` values of `[10, 100, 1000, 10000]`.\n",
    "   - `ElasticNet`: `alpha` values of `[0.01, 0.1, 1, 10]`, `l1_ratio` values of `[0.2, 0.5, 0.8]`, and `max_iter` set to `[10000]`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ec5124-13e2-4b86-bbb3-be0e9f93df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and hyperparameter grid\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, LinearRegression\n",
    "\n",
    "models = {\n",
    "    'Lasso': Lasso(),\n",
    "    'Ridge': Ridge(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'Lasso': {\n",
    "                'model__alpha': [10, 100, 1000, 10000],\n",
    "                'model__max_iter': [10000]\n",
    "        },\n",
    "    'Ridge': {'model__alpha': [10, 100, 1000, 10000]\n",
    "        },\n",
    "    'ElasticNet': {\n",
    "        'model__alpha': [0.01, 0.1, 1, 10],\n",
    "        'model__l1_ratio': [0.2, 0.5, 0.8],\n",
    "        'model__max_iter': [10000]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99712bd-0df3-4f94-856a-eb961555fdea",
   "metadata": {},
   "source": [
    "### C. Perform Cross-Validation and Hyperparameter Tuning\n",
    "**Instructions:**\n",
    "1. Import the necessary components for cross-validation and hyperparameter tuning:\n",
    "   - `Pipeline` from `sklearn.pipeline`\n",
    "   - `GridSearchCV` from `sklearn.model_selection`\n",
    "2. Create an empty dictionary called `best_models` to store the best-tuned models.\n",
    "3. Using a `for` loop, iterate over the  model name and model instance in the `models` dictionary. To iterate over a dictionary, use `.items()`:\n",
    "   - Create a `Pipeline()` called `pipeline`. Within the `Pipeline()`, pass in the model instance from the `models` dictionary that you are iterating over. \n",
    "   - Create an instance of `GridSearchCV()` called `grid_search` to perform hyperparameter tuning:\n",
    "      - Pass in the pipeline \n",
    "      - Pass in corresponding hyperparameter grid from `param_grid` \n",
    "      - Use 5-fold cross-validation \n",
    "      - Set `scoring` to `'neg_mean_squared_error'` to optimize for minimizing mean squared error.\n",
    "   - Fit `GridSearchCV` to the training data, `X_train` and `y_train` \n",
    "   - Store the best estimator from grid_search called `best_estimator_` in `best_models`.\n",
    "   - Print the best hyperparameters for each model.\n",
    "4. After finding the best models, retrain each model in `best_models` on the full training data (`X_train`, `y_train`) using a `for` loop that iterates over the name and model in `best_models.items()` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bc0da59-ae37-4427-a972-c511e690c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Lasso: {'model__alpha': 1000, 'model__max_iter': 10000}\n",
      "Best parameters for Ridge: {'model__alpha': 1000}\n",
      "Best parameters for ElasticNet: {'model__alpha': 1, 'model__l1_ratio': 0.8, 'model__max_iter': 10000}\n"
     ]
    }
   ],
   "source": [
    "# Import Functions  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create empty best models dictionary \n",
    "best_models = {}\n",
    "\n",
    "# Loop over models dictionary \n",
    "for name, model in models.items():\n",
    "    # Create pipeline object \n",
    "    pipeline = Pipeline([('model', model)])\n",
    "    \n",
    "    # Create GridSearchCV object, passing in pipeline and param_grid[name] \n",
    "    grid_search = GridSearchCV(pipeline, param_grid[name], cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Report Best Model/Parameters \n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Fit Best Models \n",
    "for name, model in best_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b35259-e654-4f23-bd29-11f0fe2f2f9a",
   "metadata": {},
   "source": [
    "### D. Evaluate Models on the Test Set\n",
    "\n",
    "**Question:**  \n",
    "After tuning our models using cross-validation, we now want to assess their performance on the test set and compare them to baseline (unoptimized) models.\n",
    "\n",
    "1. Use the best-tuned models stored in `best_models` to make predictions on `X_test`.\n",
    "2. Compute the root mean squared error (RMSE) for each optimized model using `mean_squared_error` from `sklearn.metrics`.  \n",
    "   - Store the RMSE values in a dictionary called `rmse_results`, with keys formatted as `\"Optimized {Model Name}\"`.\n",
    "3. To establish a performance baseline, evaluate **unoptimized models**:\n",
    "   - Define a dictionary `unoptimized_models` containing:\n",
    "     - `Lasso` with `alpha=1.0`\n",
    "     - `Ridge` with `alpha=1.0`\n",
    "     - `LinearRegression` as a reference model\n",
    "   - Train each model on `X_train` and predict on `X_test`.\n",
    "   - Compute and store their RMSE values in `rmse_results` with appropriate labels.\n",
    "4. Compare the RMSE values of the optimized vs. unoptimized models:\n",
    "   - Which models improved the most after hyperparameter tuning?\n",
    "   - Did any models perform worse after tuning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e94c76fc-fb2e-4e6f-8f1f-11a7a9d5e57d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Lasso RMSE on test set: 29925.28\n",
      "Optimized Ridge RMSE on test set: 28148.74\n",
      "Optimized ElasticNet RMSE on test set: 28666.65\n",
      "Unoptimized Lasso RMSE on test set: 30418.04\n",
      "Unoptimized Ridge RMSE on test set: 30383.56\n",
      "Linear Regression RMSE on test set: 30397.98\n",
      "\n",
      "RMSE Comparison (in dollars):\n",
      "Linear Regression: $30,398\n",
      "Unoptimized Lasso: $30,418\n",
      "Optimized Lasso: $29,925\n",
      "Unoptimized Ridge: $30,384\n",
      "Optimized Ridge: $28,149\n",
      "Optimized ElasticNet: $28,667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.800e+10, tolerance: 1.574e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Import Function \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create dictionary of RMSE for optimized models \n",
    "rmse_results = {}\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    rmse_results[f\"Optimized {name}\"] = rmse\n",
    "    # print(f\"{name} RMSE on test set: {rmse:.2f}\")\n",
    "    print(f\"Optimized {name} RMSE on test set: {rmse:.2f}\")\n",
    "# Compare with unoptimized models\n",
    "unoptimized_models = {\n",
    "    'Unoptimized Lasso': Lasso(alpha=1.0),\n",
    "    'Unoptimized Ridge': Ridge(alpha=1.0),\n",
    "    'Linear Regression': LinearRegression()\n",
    "}\n",
    "\n",
    "for name, model in unoptimized_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    rmse_results[name] = rmse\n",
    "    print(f\"{name} RMSE on test set: {rmse:.2f}\")\n",
    "\n",
    "# Define print order\n",
    "print_order = [\n",
    "    \"Linear Regression\",\n",
    "    \"Unoptimized Lasso\",\n",
    "    \"Optimized Lasso\",\n",
    "    \"Unoptimized Ridge\",\n",
    "    \"Optimized Ridge\",\n",
    "    \"Optimized ElasticNet\"\n",
    "]\n",
    "\n",
    "# Print Results \n",
    "print(\"\\nRMSE Comparison (in dollars):\")\n",
    "for model in print_order:\n",
    "    if model in rmse_results:\n",
    "        print(f\"{model}: ${rmse_results[model]:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6e8e2-3cd2-4450-8bce-3b462f3c6bde",
   "metadata": {},
   "source": [
    "### E. Qualitative Questions \n",
    "1. **Why do we use cross-validation instead of simply evaluating the model once on the training data? How does cross-validation help us assess model performance more reliably?**  \n",
    "2. **What does GridSearchCV do conceptually? Why is it important to tune hyperparameters rather than using default values?**  \n",
    "3. **In this problem set, we tested both optimized (tuned) and unoptimized (default) models. What patterns do you notice when comparing their performance on the test set? Does hyperparameter tuning always improve performance? Why or why not?**  \n",
    "4. **In the param_grid dictionary, we varied alpha for Ridge and Lasso regression. What does alpha control in these models? How would an extremely large or small value of alpha affect the model's predictions?**  \n",
    "5. **What are some reasons we might prefer to choose Lasso over Ridge or vice-versa?**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3769dd6",
   "metadata": {},
   "source": [
    "1. Cross validation helps to avoid overfitting. Using only the training data to test a model may come out as perfectly accurate but it will not be able to handle real world data, or any data outside of the dataset it was trained on. Cross validation also rotates which subset it's using for reducing error, so we know that every point will be used.\n",
    "2. GridSearchCV runs through the various settings nd tests each one, to find the one with the lowest error. Our default values are just the start so we need to find that balance in terms of complexity, what to prioritize and what to value, and this depends on noise levels.\n",
    "3. I noticed that the lower RMSE values correlate with the optimized models. This is due to the alpha finidng balance to find the closest values while being able to generalize. If the increased complexity leads to overfitting, then the tuning might show worse results to compared to the unoptimized model.\n",
    "4. Alpha, in this case of param_grid, is the regularization strength and concrols the penalty of the size of the coefficients. An extremely large alpha will penalize the coefficients, and will head towards zero, resulting in lower complexity and possible underfitting of the model. An extremely small value of alpha will make the model too sensitive to the training data noise having high variance.\n",
    "5. A reason we might prefer to choose Lasso is when we believe only a few of the features will make the difference in prediction, when the features are driving coefficients to zero, this basically selects the correct features to focus on. We might prefer Ridge when we have many features that contribute similarily to the end product or calculations, Ridge will not usually cause a coefficient to be pushed to zero.\n",
    "\n",
    "Gemini Transcript: https://gemini.google.com/share/cbf718eec538\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
